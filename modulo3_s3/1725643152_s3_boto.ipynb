{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Buckets s3\n",
    "## Servicios para almacenamiento de documentos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "\n",
    "- Archivo CSV:  [Homicidios_transito_ponal.csv](https://www.datos.gov.co/Seguridad-y-Defensa/Homicidios-accidente-de-tr-nsito-Polic-a-Nacional/ha6j-pa2r)\n",
    "- Página web de prueba : [Descargar](webpage/pagina.zip)\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "10_min"
    ]
   },
   "source": [
    "## Introducción\n",
    "\n",
    "Amazon Web Services (AWS) es una plataforma de servicios en la nube que ofrece una variedad de infraestructuras como servicio (IaaS), plataforma como servicio (PaaS) y software como servicio (SaaS). Ofrece servicios en diversas áreas de la computación en la nube, incluyendo almacenamiento de datos, cómputo, networking, base de datos, análisis de datos, aprendizaje automático, IoT, seguridad y mucho más.\n",
    "\n",
    "**Uno de los servicios más utilizados y esenciales de AWS es el Simple Storage Service (S3)**. AWS S3 es un servicio de almacenamiento de objetos diseñado para almacenar y recuperar cualquier cantidad de datos desde cualquier lugar: sitios web, aplicaciones móviles, aplicaciones corporativas y datos de sensores o dispositivos IoT. Es una solución ideal para respaldar y restaurar datos, archivar, y mucho más debido a su durabilidad, seguridad y escalabilidad.\n",
    "\n",
    "**S3 funciona en torno al concepto de \"buckets\" y \"objetos\"**. Un bucket es un contenedor de almacenamiento para los objetos, que son fundamentalmente los archivos que subes a S3. Cada objeto consta de datos, un identificador clave (nombre) y metadatos.\n",
    "\n",
    "**Un concepto interesante y útil cuando se habla de almacenamiento en la nube y big data es el \"datalake\"**. Un datalake es un sistema o repositorio de almacenamiento que almacena grandes cantidades de datos en su formato nativo, generalmente en objetos de tipo archivo y a veces en esquemas semi-estructurados o no estructurados.\n",
    "\n",
    "S3 puede ser utilizado como un datalake porque permite el almacenamiento masivo y escalable de datos en varios formatos, incluyendo CSV, JSON, Parquet, imágenes, videos, etc. Además, S3 permite la integración con otras herramientas de análisis de datos de AWS como Athena, Redshift, Quicksight y EMR para proporcionar capacidades de procesamiento y análisis de big data en los datos almacenados en el datalake.\n",
    "\n",
    "**Un datalake en S3 permite almacenar todos los datos en un solo lugar, independientemente del volumen y la variedad de los datos, lo que facilita el acceso y análisis de datos a lo largo del tiempo, ya que los datos no necesitan ser transformados ni cargados en un sistema separado para su análisis**. La capacidad de almacenar grandes cantidades de datos y el potencial para realizar análisis sofisticados hacen de S3 una opción popular para los datalakes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivo\n",
    "\n",
    "Esta guía presenta la librería boto3, una de las principales opciones **para interactuar con algunos de los servicios de Amazon Web Services**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo primero que debemos hacer es crear un servicio de S3 en AWS y configurar las credenciales de acceso. Ingrese a la consola de AWS y busque el servicio S3\n",
    "\n",
    "![AWS_S3.png](imagenes/AWS_S3.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seleccione la opción Create Bucket\n",
    "\n",
    "![AWS_CreateBucket.png](imagenes/AWS_CreateBucket.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Siga las instrucciones del sistema de AWS para los servicios de S3, asigne un nombre al bucket\n",
    "\n",
    "![AWS_S3_instrucciones.png](imagenes/AWS_S3_instrucciones.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En general puede configurar el bucket con las opciones por defecto que se presentan, en este caso solo vamos a realizar una modificación, en la sección de acceso público, eliminando el bloqueo para permitir que el acceso al bucket sea libre.\n",
    "\n",
    "![AWS_S3_publicaccess.png](imagenes/AWS_S3_publicaccess.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posteriormente debemos configurar las credenciales de acceso a AWS. Para realizar este proceso, ingrese a las opciones de IAM (Manejo de Identidades)\n",
    "\n",
    "![AWS_IAMService.png](imagenes/AWS_IAMService.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dentro de los servicios de Manejo de Identidad (IAM), seleccione Users >> Create User.\n",
    "\n",
    "![alt text](imagenes/AWSCreateuser.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Siga las instrucciones de la consola de administración:\n",
    "- Asigne un nombre\n",
    "- Si no cuenta con un grupo de usuarios, seleccione la opción **Create Group**\n",
    "- En la creación del grupo, asigne un nombre y escoja Amazon S3 Full Access en los permisos\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![AWSS3_IAMPermissions.png](imagenes/AWSS3_IAMPermissions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```{admonition} Importante\n",
    "\n",
    "Si esta desarrollando esta guía con la cuenta de AWS Academy es posible que la creación de cuentas de usuario esté restringida. Siga los siguientes pasos para determinar sus credenciales de acceso.\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "Seleccione en el Academy Learner Lab la ocpicón **AWS Details** y junto al texto AWS CLI el boton **Show**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](imagenes/awsacademycredentials.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "aws_access_key_id=\"ASIA4LJBLNESNJQDNBFY\"\n",
    "aws_secret_access_key=\"f4jPT3baFrUWyEQPAth+KeZHOKsxinERaoyaf0cT\"\n",
    "aws_session_token=\"IQoJb3JpZ2luX2VjEMv//////////wEaCXVzLXdlc3QtMiJHMEUCIQDsBMDRny6nNume3LblABz+0apnZQKTC/dAY/1MlxLIPAIgLDfriziMHgeBsfLizB7ahd/aDjWg1iH3k6AwAW0dnm4qrwII5P//////////ARAAGgw4NDg4NjI4NjU3MDAiDH8JJ/ShOlevbIkyMiqDAt5VYrcXh8mmYnut4tbE62QMOBXkgKpH7Ggh1lQXQo94dnJMLLztj9bJmc3d/fchU0q2RKqUxG+XcYnGz6AxG7Lu2YRlMgvsVy2D0seCmVRvWW9AB8uk79nktJDAD5slEc7ZhAcKCQpJTKSTdGcWFm3l7BJSw32cfIG+6kfiQb9CxGJsB80KMd6uQYsfuj+rXIEaXfErbntGb65HgAICvNPTESaNfXVd+8gxFF8kleKv7acZcgtJsJU432hNsyNR25OHnKVKlAXPqHpog9jukNbunBd0bWDfZ10Ml/pBXjHnCoEg54vEoZsWg3R8L38frAkYjRp5s4x6WwID+UzpEIlCek8w4sTktgY6nQHTp04nLuz+FHbTbDEnhT6AhPG/yShvCFhIlzIpH0j/Dm+FXUV6yiZQZlA69qSdbpKmlVEBPrleqJfJa2emS/qdwSkae4fqXg7Dhjk+aSDfdVPkx/00E9mKSdI1bpvBBrLLRo2rZut09zbpH4aXzlSJAzYnWTYSdCNb7OLnxLuDs/hHWJ6SkWMOJRweoHJ8u7qee7oazKXPi+2s9VdG\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta guía se manejan variables directamente definidas en el código, sin embargo en un entorno de producción es recomendable realizar a través de archivos en ubicaciones específicas como `~/.aws/credentials` y  `~/.aws/config`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_access_key_id=\"access_key\"\n",
    "aws_secret_access_key=\"secretkey\"\n",
    "aws_session_token= \"token\"\n",
    "region= 'us-east-1'\n",
    "bucket_name = 'unisabanabucket'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asegúrese de instalar la librería mediante `conda install -c conda-forge boto3` ó  `pip install boto3` en su entorno de Conda. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición de funciones\n",
    "\n",
    "Para realizar la conexión a nuestro Bucket, vamos a definir algunas funciones que permitiran la conexion mediante la librería Boto3.\n",
    "\n",
    "Inicialmente realizaremos una prueba de conexión al repositorio de documentación para conocer los permisos con que cuenta al llave de acceso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'TH9JTH5A29KG8GRK',\n",
       "  'HostId': 'wAL/3DBgkdcGaVfL0550pCrPmQ61NhFbuyY4Fjrp1VkcZGQk/SsFrqU6ByvUkOCExkpFVYzVtR4=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'wAL/3DBgkdcGaVfL0550pCrPmQ61NhFbuyY4Fjrp1VkcZGQk/SsFrqU6ByvUkOCExkpFVYzVtR4=',\n",
       "   'x-amz-request-id': 'TH9JTH5A29KG8GRK',\n",
       "   'date': 'Thu, 05 Sep 2024 03:54:55 GMT',\n",
       "   'content-type': 'application/xml',\n",
       "   'transfer-encoding': 'chunked',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 0},\n",
       " 'Owner': {'DisplayName': 'awslabsc0w6177903t1693244912',\n",
       "  'ID': '2d50fd782e0c47fd661472368b4324152ef82a4eb92b6067372e17196e87ede6'},\n",
       " 'Grants': [{'Grantee': {'DisplayName': 'awslabsc0w6177903t1693244912',\n",
       "    'ID': '2d50fd782e0c47fd661472368b4324152ef82a4eb92b6067372e17196e87ede6',\n",
       "    'Type': 'CanonicalUser'},\n",
       "   'Permission': 'FULL_CONTROL'}]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3 = boto3.client(\n",
    "    's3', \n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key,\n",
    "    aws_session_token=aws_session_token,)\n",
    "result = s3.get_bucket_acl(Bucket=bucket_name)\n",
    "result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función para subir un archivo al Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_file_to_s3(filename, bucket, object_name=None):\n",
    "    if object_name is None:\n",
    "        object_name = filename\n",
    "\n",
    "    s3_client = boto3.client('s3', \n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key,\n",
    "    aws_session_token=aws_session_token,)\n",
    "    try:\n",
    "        response = s3_client.upload_file(filename, bucket, object_name)\n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función para descargar un archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descargar un archivo\n",
    "def download_file_from_s3(filename, bucket, object_name=None):\n",
    "    if object_name is None:\n",
    "        object_name = filename\n",
    "\n",
    "    s3_client = boto3.client('s3', \n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key,\n",
    "    aws_session_token=aws_session_token)\n",
    "    try:\n",
    "        s3_client.download_file(bucket, object_name, filename)\n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```{admonition} Ejericio\n",
    "\n",
    "Desarrolle un código que devuelva el listado de los archivos disponibles en el bucket\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Ejercicio Individual\n",
    "# Esta función contiene una etiqueta remove-input para no publicarla en las guías\n",
    "\n",
    "def list_objects_in_bucket(bucket, limit):\n",
    "    s3_client = boto3.client('s3', \n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key,\n",
    "    aws_session_token=aws_session_token)\n",
    "    paginator = s3_client.get_paginator('list_objects_v2')\n",
    "    count = 0\n",
    "    for page in paginator.paginate(Bucket=bucket):\n",
    "        for object in page['Contents']:\n",
    "            if count == limit:\n",
    "                return\n",
    "            print(f'Object: {object[\"Key\"]}, Last modified: {object[\"LastModified\"]}')\n",
    "            count += 1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Prueba de funciones\n",
    "\n",
    "Ahora vamos a probar algunas de las funciones definidas previamente"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "\n",
    "Puede descargar el archivo de prueba en el enlace [Homicidios_transito_ponal.csv](https://www.datos.gov.co/Seguridad-y-Defensa/Homicidios-accidente-de-tr-nsito-Polic-a-Nacional/ha6j-pa2r)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Subir el archivo 'homicidios_transito_ponal.csv' en el directorio actual y un bucket llamado 'my-bucket'\n",
    "upload_file_to_s3('homicidios_transito_ponal.csv', bucket_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio\n",
    "Defina una función que liste todos los objetos disponibles en el bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object: homicidios_transito_ponal.csv, Last modified: 2024-09-05 03:59:28+00:00\n"
     ]
    }
   ],
   "source": [
    "# Listar todos los objetos en 'my-bucket'\n",
    "list_objects_in_bucket(bucket_name,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Descargar el archivo 'homicidios_transito_ponal.csv' del bucket 'BUCKET_NAME'\n",
    "# Realice los cambios en nombre del archivo csv y del bucket de acuerdo a lo que definió en el proceso de configuración\n",
    "download_file_from_s3('homicidios_transito_ponal.csv', bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "from botocore.exceptions import NoCredentialsError\n",
    "\n",
    "def upload_folder_to_s3(bucket_name, folder_path):\n",
    "    s3 = boto3.client('s3', \n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key,\n",
    "    aws_session_token=aws_session_token)\n",
    "\n",
    "    for subdir, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            full_path = os.path.join(subdir, file)\n",
    "            with open(full_path, 'rb') as data:\n",
    "                try:\n",
    "                    s3.upload_fileobj(data, bucket_name, full_path[len(folder_path):])\n",
    "                    print(f'Archivo {file} subido con éxito')\n",
    "                except NoCredentialsError:\n",
    "                    print(\"Las credenciales no están disponibles\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalmente no subimos o descargamos un solo archivo, ahora vamos a probar subiendo una carpeta completa.\n",
    "\n",
    "Este ejemplo no crea una subcarpeta dentro del bucket, analice como realizaría el proceso para que al subir el folder, el contenido quede almacenado en un directorio específico dentro del bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "def upload_folder_to_s3(bucket_name, directory_name):\n",
    "    s3 = boto3.resource('s3', \n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key,\n",
    "    aws_session_token=aws_session_token)\n",
    "\n",
    "    # Iterar a través de todos los archivos en la carpeta (y subcarpetas)\n",
    "    for root, dirs, files in os.walk(directory_name):\n",
    "        for filename in files:\n",
    "\n",
    "            # Construir la ruta completa del archivo\n",
    "            local_path = os.path.join(root, filename)\n",
    "\n",
    "            # Construir la ruta completa del archivo en el bucket S3\n",
    "            # Nota: esto asume que 'directory_name' es una carpeta en el directorio actual.\n",
    "            # Si 'directory_name' es en realidad una ruta completa, entonces puedes cambiar la siguiente línea a:\n",
    "            # s3_path = os.path.relpath(local_path)\n",
    "            s3_path = os.path.relpath(local_path, directory_name)\n",
    "\n",
    "            try:\n",
    "                s3.meta.client.upload_file(Filename=local_path, Bucket=bucket_name, Key=s3_path)\n",
    "                print(f\"Archivo {filename} subido a S3 con éxito\")\n",
    "            except Exception as e:\n",
    "                print(f\"No se pudo subir el archivo {filename} a S3: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo pagina.zip subido a S3 con éxito\n",
      "Archivo index.html subido a S3 con éxito\n"
     ]
    }
   ],
   "source": [
    "\n",
    "upload_folder_to_s3(bucket_name, 'webpage')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Otros opciones de Boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defina las credenciales de acceso de un usuario que cuente con permisos para lectura en el servicio EC2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Crea una sesión en la región que deseas consultar\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id='TU_ACCESS_KEY',\n",
    "    aws_secret_access_key='TU_SECRET_KEY',\n",
    "    region_name='us-east-1'  # Cambia a la región que necesites\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Conectarse al servicio EC2\n",
    "ec2 = session.client('ec2')\n",
    "\n",
    "# Obtener una lista de todas las instancias disponibles\n",
    "response = ec2.describe_instances()\n",
    "\n",
    "# Procesar la respuesta y mostrar información de las instancias\n",
    "for reservation in response['Reservations']:\n",
    "    for instance in reservation['Instances']:\n",
    "        print(f\"Instance ID: {instance['InstanceId']}\")\n",
    "        print(f\"State: {instance['State']['Name']}\")\n",
    "        print(f\"Instance Type: {instance['InstanceType']}\")\n",
    "        print(f\"Public IP: {instance.get('PublicIpAddress', 'No Public IP')}\")\n",
    "        print(f\"Launch Time: {instance['LaunchTime']}\")\n",
    "        print(\"-\" * 20)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "A través  de este notebook, hemos revisado las opciones de AWS y su servicio S3 para almacenar archivos de cualquier tipo.\n",
    "\n",
    "Este proceso se realizó mediante la librería boto3 que no solo tiene métodos para interactuar con el servicio S3, para conocer algunas funcionalidades adicionales de esta librería, puede consultar la información disponible en [AWS](https://aws.amazon.com/es/sdk-for-python/)"
   ]
  }
 ],
 "metadata": {
  "c1_recart": "7.17.0-57c20131aabc1dc2a8c675852d80a7da",
  "kernelspec": {
   "display_name": "USAB_CLOUD101",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
